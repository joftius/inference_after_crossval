\documentclass{beamer}

\mode<presentation> {

  % The Beamer class comes with a number of default slide themes
  % which change the colors and layouts of slides. Below this is a list
  % of all the themes, uncomment each in turn to see what they look like.

  % \usetheme{default}
  % \usetheme{AnnArbor}
  % \usetheme{Antibes}
  % \usetheme{Bergen}
  % \usetheme{Berkeley}
  % \usetheme{Berlin}
   \usetheme{Boadilla}
  % \usetheme{CambridgeUS}
  % \usetheme{Copenhagen}
  % \usetheme{Darmstadt}
  % \usetheme{Dresden}
  % \usetheme{Frankfurt}
  % \usetheme{Goettingen}
  % \usetheme{Hannover}
  % \usetheme{Ilmenau}
  % \usetheme{JuanLesPins}
  % \usetheme{Luebeck}
  % \usetheme{Madrid}
  % \usetheme{Malmoe}
  % \usetheme{Marburg}
  % \usetheme{Montpellier}
  % \usetheme{PaloAlto}
  % \usetheme{Pittsburgh}
  % \usetheme{Rochester}
  % \usetheme{Singapore}
  % \usetheme{Szeged}
  % \usetheme{Warsaw}

  % As well as themes, the Beamer class has a number of color themes
  % for any slide theme. Uncomment each of these in turn to see how it
  % changes the colors of your current slide theme.

  % \usecolortheme{albatross}
  % \usecolortheme{beaver}
  % \usecolortheme{beetle}
  % \usecolortheme{crane}
  % \usecolortheme{dolphin}
  % \usecolortheme{dove}
  % \usecolortheme{fly}
  % \usecolortheme{lily}
  % \usecolortheme{orchid}
  % \usecolortheme{rose}
  % \usecolortheme{seagull}
  % \usecolortheme{seahorse}
  % \usecolortheme{whale}
  % \usecolortheme{wolverine}

  % \setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
  % \setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

  % \setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and
                      % \bottomrule in tables
\usepackage{bbm}
\usepackage{bm}
\newcommand{\Kappa}{\mathrm{K}}
\newcommand{\Beta}{\mathrm{B}}
\newcommand{\mc}{\mathcal}
\newcommand{\bb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\tr}{\text{tr}}
\newcommand{\te}{\text{te}}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{amsmath,amsthm,amssymb}
% \newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
% \newtheorem{lemma}{Lemma}
% \newtheorem{corollary}{Corollary}
% \newtheorem{conjecture}{Conjecture}
\newcommand{\E}{\mathbb E}
\usepackage[authoryear]{natbib}
\newtheorem*{assumption*}{\assumptionnumber}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\argmin}{arg\,min}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]
 {%
  \renewcommand{\assumptionnumber}{Assumption #1 (#2)}%
  \begin{assumption*}%
  \protected@edef\@currentlabel{#1 (#2)}%
 }
 {%
  \end{assumption*}
 }
\makeatother

% ----------------------------------------------------------------------------------------
% TITLE PAGE
% ----------------------------------------------------------------------------------------

\title[CV inference]{Selective inference after cross-validation}

\author[J. R. Loftus]{Joshua R. Loftus}

\date

\begin{document}

\begin{frame}
  \titlepage % Print the title page as the first slide
\end{frame}


% Beginning with Lockhart et al. (2014), a surge of recent research has focused on inference after model selection using a framework of conditioning on the selected model. We outline a general procedure that describes model selection in terms of quadratic inequalities and illustrate the utility of this framework with several examples including group lasso, forward stepwise regression, and k-means clustering. Time permitting, we will discuss the interpretation and optimality properties of these conditional inference techniques described in Fithian et al. (2014), and some recent headway in the case where the error variance is unknown. Taken together, this line of inquiry is yielding a rigorous and interpretable new paradigm for post-selection inference.

\section{Cross-validation}
\label{sec:cv}

\begin{frame}{RSS test error estimation}

  \begin{itemize}

  \item For $K$-fold cv, data partitioned (randomly) into $D_1,
    \ldots, D_K$. For each $k = 1,\ldots,K$, hold out $D_k$ as a test
    set while training a model on the other $K-1$ folds. Form estimate
    $RSS_k$ of out-of-sample prediction error. Average these estimates
    over test folds. 

  \item Use to choose model complexity: evaluate $RSS_{k,s}$ for
    various sparsity choices $s$. Pick $s$ minimizing the cv-RSS estimate.

  \end{itemize}
  
\end{frame}

\begin{frame}{Examples}
  
  For each training set...

  \begin{itemize}

  \item Train LASSO models on a grid of $\lambda$ values. Or fit
    sequentially with \textsc{glmnet}. Choose $\lambda^*$ minimizing
    cv-RSS. Finally, fit a LASSO model at $\lambda^*$ on the
    whole data. 

  \item Run forward stepwise with maxsteps $S$. For $s = 1,\ldots,
    S$ evaluate the test error $RSS_{k,s}$. Average to
    get $RSS_s$. Pick $s^*$ minimizing this. Run forward stepwise on the
    whole data for $s^*$ steps. 

  \end{itemize}

  Can we do selective inference for the final models chosen this way?

\end{frame}

\begin{frame}{Quadratic constraints example: forward stepwise}

  \begin{block}{Key observation}
    The inequalities $\text{RSS}(i_s) \leq \text{RSS}(j)$ characterizing the variable added at step $s$ can all be written in the form
    \[
    y^T(Q_{i_s} - Q_{j,s})y \geq 0 \quad \forall j \neq i_s
    \]
  \end{block}

  
  Define $E_{s,j} := \{ y : y^T (Q_{i_s} - Q_{j,s} )y \geq 0 \}$.
  
  \begin{block}{Characterization of the selection event}
    The event $E$ that forward stepwise chooses model $m$ can be
    written
    \[
      E = \bigcap_{s=1}^S \bigcap_{j \neq i_s, \ldots, i_1} E_{s,j}
    \]
  \end{block}

\end{frame}

\begin{frame}{Reduction to one dimension}
  
  \begin{itemize}

  \item The geometry of $E$ (intersection of quadratics) is
    complicated. For example, verifying if an ellipsoid contains an
    intersection of ellipsoids is NP-complete (Boyd et. al. LMI
    book, Section 3.7.2). Our quadratics may not even be $\succeq 0$. 

  \item To test for variable $i_s$, reduce to a one-dimensional
    problem. Define $T\chi_{i_s} = \| P y \|_2$ for some projection
    $P$. Let $U = Py/S$ and $Z = y-Py$. Support of $T\chi_{i_s}$
    \[
    M_{i_s} = \{ t \geq 0 : Ut+Z \in E \}
    \] 

  \end{itemize}

\end{frame}

\begin{frame}{Cartoon of selection event and one-dimensional slice}

  \includegraphics[width=\textwidth]{leonard.jpeg}

\end{frame}

\begin{frame}{Cross-validation}
  
  \begin{itemize}

  \item Let $f, g$ index CV test folds. 

  \item On fold $f$, model $m_f$ at step $s$, and $-f$ denoting the
    training set for test fold $f$ (complement of $f$). 

  \item Define $P_{f,s} := X^f_{m_f,s}(X^{-f}_{m_f,s})^\dagger$ (not a projection)

  \item $s = \argmin_s \sum_{f=1}^K \| y^{f} - P_{f,s} y^{-f} \|_2^2$ 

  \item Sums of squares... maybe it's a quadratic form?

  \end{itemize}

\end{frame}

\begin{frame}{The key result}
  
  \begin{block}{Blockwise quadratic form of cv-RSS}
    Define $Q^s_{ff} := \sum_{g\neq f}(P_{g,s})_f^T(P_{g,s})_f$ and
    \[
      Q^s_{fg} := -(P_{f,s})_g - (P_{g,s})^T_f + \sum_{\substack{h=1\\h \notin \{f, g \}}}^K (P_{h,s})^T_f (P_{h,s})^T_g
    \]
     Then with $y_K$ denoting the observations ordered by CV-folds, 
    \[
    \text{cv-RSS}(s) = y_K^TQ^sy_K
    \]
  \end{block}
  
  \begin{proof}
    Algebra
  \end{proof}

\end{frame}

\section{Example: forward stepwise}
\label{sec:fs}

\begin{frame}{Does it work?}
  
  Well yes, it's a theorem. 

  \  

  End of talk. Thank you for listening!


\end{frame}

\begin{frame}{Global null, K = 5, n = 50, p = 100, steps = 10}
  
  \begin{columns}[c]
    \column{.7\textwidth}
    \includegraphics[width=8cm,
    height=8cm]{../../../r/josh/cvresults/results_cv_n50_p100_sparsity0_snr0_bystep.pdf}
    \column{.3\textwidth}
    About 16\% chose sparsity $> 1$ \\ \  \\ Less than 0.5\% chose sparsity $> 3$
  \end{columns}

\end{frame}

\begin{frame}{5-sparse, K = 5, SNR = 7, n = 50, p = 100, steps = 10}
  
  \begin{columns}[c]
    \column{.7\textwidth}
    \includegraphics[width=8cm, height=8cm]{../../../r/josh/cvresults/results_cv_n50_p100_sparsity5_snr1_bynull.pdf}
    \column{.3\textwidth}
    Screened 90\% \\ \  \\ Another 4\% 4/5
  \end{columns}
    
\end{frame}

\section{Conclusion}
\label{sec:future}

\begin{frame}{Limitations / ongoing work}

  \begin{itemize}

  \item Forward stepwise used linearity $\hat y^f = P y^{-f}$. Ridge
    has same form. Lasso has additional constant terms, but 
    works the same otherwise.

  \item Computationally expensive. Might be able to optimize a little more. 

  \item Tuning parameters: $K$ and $\text{maxsteps}$ (or $\lambda$
    grid).

  \item Unknown $\sigma$. Estimate by CV / use selective $F$ test. 

  \end{itemize}
  
\end{frame}

\end{document}